{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6959809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable\n",
    "\n",
    "MIN: int = 0\n",
    "MAX: int = 4\n",
    "BINS: int = 100\n",
    "NUM_SAMPLES: int = 100000\n",
    "N: int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy language model that returns a uniformly distributed random number\n",
    "def model(num_samples) -> np.ndarray:\n",
    "    return np.random.uniform(0, MAX, num_samples)\n",
    "\n",
    "def histogram(output: list[int]):\n",
    "    hist, bins = np.histogram(output, bins=BINS, range=(MIN, MAX), density=False)\n",
    "    probs = hist / np.sum(hist)\n",
    "    return probs, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e317e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ground truth reward model. We assume that we have a preference for the number `mid`.\n",
    "def reward_model_ground_truth(output) -> float:\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "# Definition of the proxy reward model. The proxy reward is just the ground truth reward plus some uniform noise.\n",
    "def reward_model_proxy(output) -> float:\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards() -> None:\n",
    "    outputs = np.linspace(MIN, MAX, 1000)\n",
    "    rewards_ground_truth = [reward_model_ground_truth(output) for output in outputs]\n",
    "    rewards_proxy = [reward_model_proxy(output) for output in outputs]\n",
    "    plt.plot(outputs, rewards_proxy, alpha=1.0)\n",
    "    plt.plot(outputs, rewards_ground_truth, alpha=1.0)\n",
    "    plt.xlabel(\"output\")\n",
    "    plt.ylabel(\"reward\")\n",
    "\n",
    "# Plot the proxy and ground truth rewards\n",
    "plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c069d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_of_n(n: int, reward_model):\n",
    "    pass\n",
    "\n",
    "\n",
    "def optimized_prob_distribution(n, is_proxy):\n",
    "    actions: list[float] = []\n",
    "    for _ in range(NUM_SAMPLES):\n",
    "        if is_proxy:\n",
    "            best_output, _  = best_of_n(n, reward_model_proxy)\n",
    "        else:\n",
    "            best_output, _  = best_of_n(n, reward_model_ground_truth) # use ground truth\n",
    "        actions.append(best_output)\n",
    "    probs, bins = histogram(actions)\n",
    "    return probs, bins\n",
    "\n",
    "# Probabilities before best-of-n sampling\n",
    "probs_initial: list[int] = BINS * [1/BINS]\n",
    "\n",
    "# Probabilities after best-of-n sampling\n",
    "probs_optimized, bins = optimized_prob_distribution(n=256, is_proxy=True)\n",
    "\n",
    "def plot_optimized_output() -> None:\n",
    "    plt.hist(bins[:-1], bins, weights=probs_optimized)\n",
    "    plt.xlabel(\"output\")\n",
    "    plt.ylabel(\"prob(output)\")\n",
    "\n",
    "# Plot the output after best-of-n sampling using the proxy reward model\n",
    "plot_optimized_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The KL divergence for best-of-n sampling can be computed analytically, see page 31 https://arxiv.org/pdf/2009.01325.pdf\n",
    "def kl_divergence_analytical(n):\n",
    "    pass\n",
    "\n",
    "def kl_divergence_numerical(p, q):\n",
    "    pass\n",
    "\n",
    "# The KL divergence between the initial distribution and the optimized distribution increases with n\n",
    "for n in [2, 4, 8, 16, 32, 64, 128, 256]:\n",
    "    # todo\n",
    "    print(f\"n={n}, kl_divergence={}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_reward(n:int, reward_model: Callable) -> float:\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "rewards_ground_truth: list[float] = []\n",
    "\n",
    "RANGE_N: list[int] = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "for n in RANGE_N:\n",
    "    reward_ground_truth: float = estimate_reward(n, reward_model_ground_truth)\n",
    "    rewards_ground_truth.append(reward_ground_truth)\n",
    "\n",
    "# Plot proxy vs. ground truth rewards\n",
    "# With uniform random noise, the proxy as well as the ground truth reward are monotonically increasing\n",
    "# But thats not the case when using a real instead of a toy reward model, see https://arxiv.org/pdf/2210.10760.pdf\n",
    "plt.plot(RANGE_N, rewards_ground_truth)\n",
    "plt.xscale('log')\n",
    "plt.ylabel('reward')\n",
    "plt.xlabel('n')\n",
    "plt.legend(['proxy', 'ground truth'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56060ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why not using very large n?\n",
    "\n",
    "\n",
    "def estimate_reward(n:int, reward_model: Callable) -> float:\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "rewards_ground_truth: list[float] = []\n",
    "rewards_proxy: list[float] = []\n",
    "\n",
    "RANGE_N: list[int] = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "for n in RANGE_N:\n",
    "    reward_proxy, reward_ground_truth = estimate_reward(n, reward_model_proxy)\n",
    "    rewards_proxy.append(reward_proxy)\n",
    "    rewards_ground_truth.append(reward_ground_truth)\n",
    "\n",
    "# Plot proxy vs. ground truth rewards\n",
    "# With uniform random noise, the proxy as well as the ground truth reward are monotonically increasing\n",
    "# But thats not the case when using a real instead of a toy reward model, see https://arxiv.org/pdf/2210.10760.pdf\n",
    "plt.plot(RANGE_N, rewards_ground_truth)\n",
    "plt.plot(RANGE_N, rewards_proxy)\n",
    "plt.xscale('log')\n",
    "plt.ylabel('reward')\n",
    "plt.xlabel('n')\n",
    "plt.legend(['ground truth', 'proxy'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs521",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
